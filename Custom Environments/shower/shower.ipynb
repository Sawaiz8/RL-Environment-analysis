{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Environment\n",
    "Build an agent to give us the best shower possible\n",
    "Randomly temperature\n",
    "Optimal is 37 to 39 degrees --> Agent doesn't know it \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowerEnv(Env):\n",
    "    def __init__(self):\n",
    "        # isn't it two? Like increase or decrease temp?\n",
    "        self.action_space = Discrete(3)\n",
    "        #I think this is for the temperature\n",
    "        self.observation_space = Box(low=0, high=100, shape=(1,))\n",
    "        self.state = np.array([38 + random.randint(-3,3)]).astype(float)\n",
    "        # Episode length\n",
    "        self.shower_length = 60\n",
    "\n",
    "    def step(self, action):\n",
    "        self.state += action-1\n",
    "\n",
    "        self.shower_length -= 1\n",
    "\n",
    "        if self.state >= 37 and self.state <= 39:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        if self.shower_length <= 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        info = {}\n",
    "        truncated = False\n",
    "\n",
    "        return self.state, reward, done, info, \n",
    "\n",
    "    # This is used a lot in the gaming function of code\n",
    "    # along with the observation space and the reward space\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.array([38 + random.randint(-3,3)]).astype(float)\n",
    "        self.shower_length = 60\n",
    "\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ShowerEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.0, 100.0, (1,), float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 Score:-60\n",
      "Episode:1 Score:-50\n",
      "Episode:2 Score:-58\n",
      "Episode:3 Score:-36\n",
      "Episode:4 Score:36\n",
      "Episode:5 Score:-52\n",
      "Episode:6 Score:-48\n",
      "Episode:7 Score:-60\n",
      "Episode:8 Score:-52\n",
      "Episode:9 Score:-50\n",
      "Episode:10 Score:-26\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(episodes+1):\n",
    "    obs = env.reset()\n",
    "    terminated = False\n",
    "    score = 0\n",
    "    while not terminated:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, info = env.step(action)\n",
    "        score += reward\n",
    "        if terminated:\n",
    "            observation = env.reset()\n",
    "                    \n",
    "    print(f\"Episode:{episode} Score:{score}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "log_path = os.path.join(\"training\", \"Logs\")\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log = log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to training/Logs/PPO_4\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 60       |\n",
      "|    ep_rew_mean     | -3.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 1412     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -2.91       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1069        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009843727 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.916      |\n",
      "|    explained_variance   | -0.00516    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.8        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00531    |\n",
      "|    value_loss           | 63.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -1.36       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 973         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008259596 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.924      |\n",
      "|    explained_variance   | -0.000347   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.9        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 75.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 3.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 945         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002576408 |\n",
      "|    clip_fraction        | 0.0502      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.902      |\n",
      "|    explained_variance   | -0.011      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33          |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00106    |\n",
      "|    value_loss           | 64.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 5.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 930         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009588653 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.925      |\n",
      "|    explained_variance   | 0.0011      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | 0.000431    |\n",
      "|    value_loss           | 51.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 60         |\n",
      "|    ep_rew_mean          | 9.5        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 917        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01197777 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.922     |\n",
      "|    explained_variance   | 0.00613    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 23         |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    value_loss           | 57         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 12.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 904         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009881358 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.895      |\n",
      "|    explained_variance   | 0.00653     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.000209   |\n",
      "|    value_loss           | 42.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 18.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 905         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008496236 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.897      |\n",
      "|    explained_variance   | 0.00839     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.2        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.000766   |\n",
      "|    value_loss           | 47.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 23           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 901          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0089016985 |\n",
      "|    clip_fraction        | 0.132        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.907       |\n",
      "|    explained_variance   | -0.021       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.3         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00736     |\n",
      "|    value_loss           | 47.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 26.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 899          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070294375 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.883       |\n",
      "|    explained_variance   | -0.000475    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.5         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00467     |\n",
      "|    value_loss           | 42.5         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 60         |\n",
      "|    ep_rew_mean          | 30.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 897        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00833976 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.856     |\n",
      "|    explained_variance   | 0.000623   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 25.2       |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.00208   |\n",
      "|    value_loss           | 51         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 31.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 897         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007347515 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.821      |\n",
      "|    explained_variance   | -0.00138    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.9        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00532    |\n",
      "|    value_loss           | 59          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 36.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 898          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036276565 |\n",
      "|    clip_fraction        | 0.0987       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.813       |\n",
      "|    explained_variance   | 0.000156     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23           |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | 0.000302     |\n",
      "|    value_loss           | 55.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 38.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 897         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009847549 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.828      |\n",
      "|    explained_variance   | 0.000241    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.9        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | 0.00172     |\n",
      "|    value_loss           | 59.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 37.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 895          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062669134 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.854       |\n",
      "|    explained_variance   | -2.69e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27           |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | 0.00253      |\n",
      "|    value_loss           | 62.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 38.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 894         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022712305 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.863      |\n",
      "|    explained_variance   | -0.000107   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36          |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00354    |\n",
      "|    value_loss           | 68.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 41.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 891          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043193204 |\n",
      "|    clip_fraction        | 0.135        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.863       |\n",
      "|    explained_variance   | 1.37e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25           |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | 0.00353      |\n",
      "|    value_loss           | 67.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 46.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 892         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008394783 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.858      |\n",
      "|    explained_variance   | 1.06e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 54          |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | 0.00848     |\n",
      "|    value_loss           | 74          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 47.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 891         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008724824 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.841      |\n",
      "|    explained_variance   | 6.5e-06     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46          |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | 0.00552     |\n",
      "|    value_loss           | 78.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 44.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 889         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008230443 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.833      |\n",
      "|    explained_variance   | -1.03e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.8        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | 0.00762     |\n",
      "|    value_loss           | 81.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f867406a370>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(\"training\", \"Saved Models\", \"Shower Model PPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sawaiz/anaconda3/envs/rl/lib/python3.9/site-packages/stable_baselines3/common/save_util.py:278: UserWarning: Path 'training/Saved Models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "011c4107da44cd2ef66b6c99b30e8675713feb7e3da5c97c6c9235cb6322cffb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
